import os
import json
import tqdm
import numpy as np
import torch
import argparse

from datasets import Dataset
import pdb
from typing import List, Dict
from functools import partial
from transformers import (
    AutoModel,
    AutoTokenizer,
    PreTrainedTokenizerFast,
    BatchEncoding,
    DataCollatorWithPadding,
)
from transformers.modeling_outputs import BaseModelOutput
from torch.utils.data import DataLoader
from mteb import MTEB

import torch
import logging

from torch import Tensor
from transformers import PreTrainedTokenizerFast, BatchEncoding
from typing import Mapping, Dict, List


def _setup_logger():
    log_format = logging.Formatter("[%(asctime)s %(levelname)s] %(message)s")
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    console_handler = logging.StreamHandler()
    console_handler.setFormatter(log_format)
    logger.handlers = [console_handler]

    return logger


logger = _setup_logger()


def move_to_cuda(sample):
    if len(sample) == 0:
        return {}

    def _move_to_cuda(maybe_tensor):
        if torch.is_tensor(maybe_tensor):
            return maybe_tensor.cuda(non_blocking=True)
        elif isinstance(maybe_tensor, dict):
            return {key: _move_to_cuda(value) for key, value in maybe_tensor.items()}
        elif isinstance(maybe_tensor, list):
            return [_move_to_cuda(x) for x in maybe_tensor]
        elif isinstance(maybe_tensor, tuple):
            return tuple([_move_to_cuda(x) for x in maybe_tensor])
        elif isinstance(maybe_tensor, Mapping):
            return type(maybe_tensor)(
                {k: _move_to_cuda(v) for k, v in maybe_tensor.items()}
            )
        else:
            return maybe_tensor

    return _move_to_cuda(sample)


def pool(last_hidden_states: Tensor, attention_mask: Tensor, pool_type: str) -> Tensor:
    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)

    if pool_type == "avg":
        emb = last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]
    elif (
        pool_type == "weightedavg"
    ):  # position-weighted mean pooling from SGPT (https://arxiv.org/abs/2202.08904)
        attention_mask *= attention_mask.cumsum(dim=1)  # [0,1,1,1,0,0] -> [0,1,2,3,0,0]
        s = torch.sum(last_hidden * attention_mask.unsqueeze(-1).float(), dim=1)
        d = attention_mask.sum(dim=1, keepdim=True).float()
        emb = s / d
    elif pool_type == "cls":
        emb = last_hidden[:, 0]
    elif pool_type == "last":
        left_padding = attention_mask[:, -1].sum() == attention_mask.shape[0]
        if left_padding:
            emb = last_hidden[:, -1]
        else:
            sequence_lengths = attention_mask.sum(dim=1) - 1
            batch_size = last_hidden.shape[0]
            emb = last_hidden[
                torch.arange(batch_size, device=last_hidden.device), sequence_lengths
            ]
    else:
        raise ValueError(f"pool_type {pool_type} not supported")

    return emb


parser = argparse.ArgumentParser(description="evaluation for BEIR benchmark")
parser.add_argument(
    "--model-name-or-path",
    default="bert-base-uncased",
    type=str,
    metavar="N",
    help="which model to use",
)
parser.add_argument(
    "--output-dir",
    default="tmp-outputs/",
    type=str,
    metavar="N",
    help="output directory",
)
parser.add_argument(
    "--doc-as-query", action="store_true", help="use query prefix for passages"
)
parser.add_argument("--pool-type", default="cls", help="pool type")
parser.add_argument("--batch-size", default=1024, help="batch size", type=int)


args = parser.parse_args()
logger.info("Args: {}".format(json.dumps(args.__dict__, ensure_ascii=False, indent=4)))
assert args.pool_type in ["cls", "avg"], "pool_type should be cls or avg"
assert args.output_dir, "output_dir should be set"
os.makedirs(args.output_dir, exist_ok=True)


def _transform_func(
    tokenizer: PreTrainedTokenizerFast, examples: Dict[str, List]
) -> BatchEncoding:
    return tokenizer(
        examples["contents"],
        max_length=512,
        padding=True,
        return_token_type_ids=False,
        truncation=True,
    )


class RetrievalModel:
    # Refer to the code of DRESModel for the methods to overwrite
    def __init__(self, **kwargs):
        self.encoder = AutoModel.from_pretrained(args.model_name_or_path)
        self.tokenizer = AutoTokenizer.from_pretrained(
            "Snowflake/snowflake-arctic-embed-m-v1.5"
        )
        self.gpu_count = torch.cuda.device_count()
        self.batch_size = args.batch_size
        if self.gpu_count > 1:
            self.encoder = torch.nn.DataParallel(self.encoder)

        self.encoder.cuda()
        self.encoder.eval()

    def encode_queries(self, queries: List[str], **kwargs) -> np.ndarray:
        input_texts = [
            "Represent this sentence for searching relevant passages: {}".format(q)
            for q in queries
        ]  # vs query: prompt
        return self._do_encode(input_texts)

    def encode_corpus(self, corpus: List[Dict[str, str]], **kwargs) -> np.ndarray:
        if args.doc_as_query:
            return self.encode_queries([d["text"] for d in corpus], **kwargs)

        input_texts = [
            "{} {}".format(doc.get("title", ""), doc["text"]).strip() for doc in corpus
        ]
        input_texts = ["{}".format(t) for t in input_texts]  # No doc prefix
        return self._do_encode(input_texts)

    @torch.no_grad()
    def _do_encode(self, input_texts: List[str]) -> np.ndarray:
        dataset: Dataset = Dataset.from_dict({"contents": input_texts})
        dataset.set_transform(partial(_transform_func, self.tokenizer))

        data_collator = DataCollatorWithPadding(self.tokenizer, pad_to_multiple_of=8)
        data_loader = DataLoader(
            dataset,
            batch_size=self.batch_size * self.gpu_count,
            shuffle=False,
            drop_last=False,
            num_workers=4,
            collate_fn=data_collator,
            pin_memory=True,
        )

        encoded_embeds = []
        for batch_dict in tqdm.tqdm(data_loader, desc="encoding", mininterval=10):
            batch_dict = move_to_cuda(batch_dict)

            with torch.cuda.amp.autocast():
                outputs: BaseModelOutput = self.encoder(**batch_dict)
                embeds = pool(
                    outputs.last_hidden_state,
                    batch_dict["attention_mask"],
                    args.pool_type,
                )
                encoded_embeds.append(embeds.cpu().numpy())

        return np.concatenate(encoded_embeds, axis=0)


def main():
    model = RetrievalModel()
    task_names = [
        "ArguAna",
        "ClimateFEVER",
        "CQADupstackAndroidRetrieval",
        "CQADupstackEnglishRetrieval",
        "CQADupstackGamingRetrieval",
        "CQADupstackGisRetrieval",
        "CQADupstackMathematicaRetrieval",
        "CQADupstackPhysicsRetrieval",
        "CQADupstackProgrammersRetrieval",
        "CQADupstackStatsRetrieval",
        "CQADupstackTexRetrieval",
        "CQADupstackUnixRetrieval",
        "CQADupstackWebmastersRetrieval",
        "CQADupstackWordpressRetrieval",
        "DBPedia",
        "FEVER",
        "FiQA2018",
        "HotpotQA",
        "MSMARCO",
        "NFCorpus",
        "NQ",
        "QuoraRetrieval",
        "SCIDOCS",
        "SciFact",
        "Touche2020",
        "TRECCOVID",
    ]
    # task_names = ['ArguAna', 'FiQA2018', "HotpotQA"]
    logger.info("Tasks: {}".format(task_names))

    for task in task_names:
        logger.info("Processing task: {}".format(task))

        args.doc_as_query = task in ["QuoraRetrieval"]

        evaluation = MTEB(tasks=[task], task_langs=["en"])
        evaluation.run(
            model,
            eval_splits=["test" if task not in ["MSMARCO"] else "dev"],
            output_folder=args.output_dir,
            overwrite_results=False,
        )


if __name__ == "__main__":
    main()
